seed: 42

training:
  accelerator: auto
  devices: 1
  strategy: auto
  max_epochs: 3
  gradient_clip_val: 1.0
  lr: 5.0e-05
  log_step: 100
  warmup_steps: 1000
  lr_gamma: 0.95
  weight_decay: 0.01

logging:
  log_every_n_steps: 50
  val_check_interval: 0.25
  checkpoints_dir: checkpoints
  log_dir: logs
  run_name: gpt2-tef-wikitext

dataset:
  name: wikitext
  config: wikitext-103-raw-v1
  block_size: 512
  train_batch_size: 16
  val_batch_size: 16
  num_workers: 8
  cache_dir: null

clearml:
  project_name: "token_estimation_function"
  task_name: "gpt2_tef_wikitext"
